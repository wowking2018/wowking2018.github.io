[{"content":"Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.\nDefinition of Deep Reinforcement Learning Something about definitions \u0026amp; notation Here, I recommend this textbook named Mathematical foundation of reinforcement learning for studying the fundamental concepts related to classical reinforcement learning.\n$\\mathbf{s}_{t}$ - state $\\mathbf{o}_{t}$ - observation $\\mathbf{a}_{t}$ - action $\\pi _{\\theta}({\\mathbf{a} _{t}} | \\mathbf{o} _{t})$ - policy (partially observed) - Instead, it receives an observation $\\mathbf{o} _{t}$, which might be a partial or noisy view of the true state. This policy defines the probability of agent taking action $\\mathbf{a} _{t}$ at​ the given observation. $\\pi _{\\theta}({\\mathbf{a} _{t}} | \\mathbf{s} _{t})$ - policy (fully observed) - In a fully observed environment, the agent has access to complete state $\\mathbf{s} _{t}$ of environment. The policy $\\pi _{\\theta}(\\mathbf{a} _t | \\mathbf{s} _t)$ defines the probability of agent taking action $\\mathbf{a}_t$. Among all of them, important definitions to know are the state which we denote $\\mathbf{s}_{t}$, the observation $\\mathbf{o} _{t}$ and the action $\\mathbf{a} _{t}$. Then, the observation and state could be related to one another by the following graphical model where the edge between observations and actions is policy, and state satisfies the Markov property.\nWe\u0026rsquo;ll start with something called Markov chain, which is named after Andrei Markov who was a mathematician pioneered the study of stochastic processes. The Markov chain has a very simple definition，it consists of just two things, a set of states s and a transition function, which means that the state at time $t+1$ is independent of the state at time $t-1$, one condition on the current state $\\mathbf{s}_{t}$.\nMarkov chain:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{T}}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{T}$ - transition operator $p(s_{t+1}|s_{t})$, a sort of linear operator, it can also be referred to as a transition probability or a dynamics function. It sounds like a little weird, why is it refferred to an operator?\nAnswer: This operator emphasizes how the Markov chain\u0026rsquo;s dynamics are governed: it takes the current distribution of states and produces the next distribution, much like how a function or matrix transforms inputs to outputs in mathematics. It will help us capture the essence of how states evolve over time in the Markov process.\nIf we represent the probabilities of each state at time step $t$ as a vector, we could call it $\\mu_{t,i}=p(s_{t}=i)$. Let\u0026rsquo;s say we have $n$ states, each with its own probability distribution represented by a probability vector $\\vec{\\mu _{t}}$, where $t$ represents the time step. Then, the transition probabilities as a matrix, where the $ij$-th entry is the probability of going into state $i$ if you are currently in the state $j$, the corresponding formula is $\\mathcal{T} _{i,j}= p(s _{t+1} = i | s _{t}=j)$. Now, we could express the vector of state probabilities at the next time step $\\vec{\\mu _{t+1}} = \\mathcal{T}\\vec{\\mu _{t}}$.\nHowever, the Markov chain itself doesn\u0026rsquo;t allow us to specify a decision, as it lacks the notion of actions. In order to go towards the notion of actions, we have to turn the Markov chain into a Markov decision process (MDP).\nMarkov decision process:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{T}, r}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{A}$ - action space, actions $a \\in \\mathcal{A}$ (discrete or continuous) $\\mathcal{T}$ - transition operator, it\u0026rsquo;s not a matrix any more, but a tensor! Because it includes next state, current state, and current action. $r$ - reward function, $r: \\mathcal{S}\\times \\mathcal{A} = \\lbrace (s,a)| s \\in \\mathcal{S}, a \\in \\mathcal{A} \\rbrace \\rightarrow \\mathbb{R}$, the reward function is a mapping from the Cartesian product of the state and action space into real valued numbers. Some useful tricks we mentioned earlier can still be applied here, let $\\mu_{t,j} = p(s_{t}=j)$, and we could have another vector $\\xi_{t,k}=p(a_{t}=k)$ that will denote the probability of taking some action. Also, transition operator could be written as a tensor, so $\\mathcal{T} _{i,j,k}= p(s _{t+1}=i | s _{t}=j,a _{t}=k)$ is the probability of entering state $i$ if you\u0026rsquo;re in state $j$ and take action $k$. The vector of state probabilities at the next time step will be a little bit complex,\n$$ \\mu_{t+1,i}=\\sum_{j,k}\\mathcal{T}_{i,j,k}\\mu_{t,j}\\xi_{t,k} $$ (P.S. the reason why we don't use $\\vec{}$ is that $\\mu_{t+1,i}$ as a tensor, not matrix.) Before we go to the next part, we\u0026rsquo;d like to extend this Markov decision process definition, which will allow us to bring in the notion of observations. So a partially observed Markov decision process further augments the definition within two additional objects, observation space $\\mathcal{O}$ and emission probability $\\mathcal{E}$.\nPartially observed Markov decision process:\n$\\mathcal{M} = {\\mathcal{S}, \\mathcal{A}, \\mathcal{O}, \\mathcal{T}, \\mathcal{E}, r}$ $\\mathcal{S}$ - state space, state $s \\in \\mathcal{S}$ (discrete or continuous) $\\mathcal{A}$ - action space, actions $a \\in \\mathcal{A}$ (discrete or continuous) $\\mathcal{O}$ - observation space, observations $o \\in \\mathcal{O}$ (discrete or continuous) $\\mathcal{T}$ - transition operator, a tensor! $\\mathcal{E}$ - emission probability $p(o_{t}|s_{t})$ $r$ - reward function, $r: \\mathcal{S}\\times \\mathcal{A} =\\lbrace (s,a)| s \\in \\mathcal{S}, a \\in \\mathcal{A}\\rbrace \\rightarrow \\mathbb{R}$ Goal of reinforcement learning We will talk about partially observed later, for now let\u0026rsquo;s just say our policy is conditioned on $s$, and $\\theta$ corresponds to the parameters of the policy. If policy is a kind of neural network, we could find that $\\theta$ denotes the parameters of this deep neural net. The state input policy and action go into the transition probability $p(\\mathbf{s}^{\\prime}|\\mathbf{s}, \\mathbf{a})$, which produces the next state.\nIn this process, we could write down a probability distribution over trajectories, which are sequences of states and actions by using chain rules.\n$$ \\begin{equation} \\underbrace{ p_{\\theta}(\\mathbf{s}_{1},\\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T} ) }_{ p_{\\theta}(\\tau) } = p(\\mathbf{s}_{1}) \\prod^{T}_{t=1}\\underbrace{ \\pi_{\\theta}(\\mathbf{a}_{t}|\\mathbf{s}_{t})p(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t}) }_{ \\text{Markov chain on} (\\mathbf{s},\\mathbf{a}) } \\end{equation} $$ For now we assume that our control problem has a finite horizon, implying that the decision-making task is limited to a fixed time step $T$ and then ends. We could factorize it by using the chain rule in terms of probability distribution that we've already defined. For notational brevity, sometimes $p_{\\theta}(\\mathbf{s}_{1},\\mathbf{a}_{1}, \\cdots, \\mathbf{s}_{T}, \\mathbf{a}_{T} )$ will be rewritten by $p_{\\theta}(\\tau)$. Having define the distribution, we could actually define an objective as an expected value under the trajectory distribution for reinforcement learning tasks. (P.S. this equation could be used in finite horizon situation) $$ \\begin{equation} \\theta^{*} = \\arg \\max_{\\theta} E_{\\tau \\sim p_{\\theta}(\\tau)} \\left[ \\sum_{t}r(\\mathbf{s}_{t},\\mathbf{a}_{t}) \\right] \\end{equation} $$ We would like to find the paraments $\\theta$, which is our goal. By doing this, we aim to maxmize the expected value of the sum of rewards over the trajectory. Generally, $\\pi_{\\theta}(\\mathbf{a} _{t}|\\mathbf{\\mathbf{s} _{t}})$ given $\\mathbf{s} _{t}$ allows us to get a distribution of our actions condition on states. What we can do is to group state and action together into a kind of augmented state. And now the augments states actually form a Markov chain as the following figure. We could write down the enhanced transition operator (personal nick name for this operator) in this augmented Markov chain.\n$$ p((\\mathbf{s}_{t+1},\\mathbf{a}_{t+1})|(\\mathbf{s}_{t},\\mathbf{a}_{t})) = p(\\mathbf{s}_{t+1}|\\mathbf{s}_{t},\\mathbf{a}_{t})\\pi_{\\theta}(\\mathbf{a}_{t+1},\\mathbf{s}_{t+1}) $$ Next we could do is to write the objective by linearity of expectation as the sum over time of the expected values under the state action marginal $p_{\\theta}(\\mathbf{s} _{t}, \\mathbf{a} _{t})$ in this Markov chain, cause this transition operator is product of transition and policy with good linear mathematical structure.\n$$ \\begin{equation} \\theta^{*} = \\arg \\max_{\\theta}\\sum^{T}_{t = 1} E_{(\\mathbf{s}_{t}, \\mathbf{a}_{t})\\sim p_{\\theta}(\\mathbf{s}_{t},\\mathbf{a}_{t})}[r(\\mathbf{s}_{t}, \\mathbf{a}_{t})] \\end{equation} $$ This formulation simplifies the computation by focusing solely on the expectation of rewards at each time step, rather than considering the expectation over the entire trajectory. Infinite horizon case What if $T \\rightarrow \\infty$ ?\nFirst, if that happens our objective might become not so clear. Please imagine that you have a sum of infinite positive numbers rewards, which is infinity. We need to make the objective finite, using discount will solve this problem，or choose a mathematical operation divide by $T$ so that the sum of the expectations of the marginals becomes dominated by the stationary distribution terms.\nNow, we could ask a more specific question, does $p(\\mathbf{s} _{t},\\mathbf{a} _{t})$ converge to a stationary distribution?\nIf this is possible, that means we could write the stationary distribution and it must obey this equation, $\\mu=\\mathcal{T}\\mu$. We could prove this conclusion under a few technical assumptions namely ergoicity and chain being aperiodic. Ergoicity means every state can be reached from every other state with non-zero probability, it prevents a situation where if you start in one part of the MDP, you might never reach another one. It dives us into characteristic equation in linear algebra to solve\n$$(\\mathcal{T}-\\mathbf{I})\\mu=0$$ We could find $\\mu$ by finding the eigenvector with eigenvalue one for the matrix defined by $t$. And $\\mu$ is eigenvector of $\\mathcal{T}$ with eigenvalue $1$, it always exists under ergodic and aperiodic assumption. When we consider the average reward case divided by $T$, the limit as $t$ approaches infinity will be the expected value of the reward. Formally, this is expressed as\n$$ \\begin{equation} \\begin{aligned} \\theta^{*} \u0026= \\arg \\max_{\\theta} \\frac{1}{T} \\sum_{t=1}^{T} E_{(\\mathbf{s}_{t}, \\mathbf{a}_{t}) \\sim p_{\\theta}(\\mathbf{s}, \\mathbf{a})} [r(\\mathbf{s}_{t}, \\mathbf{a}_{t})] \\rightarrow \\color{red}{(E_{(\\mathbf{s}, \\mathbf{a}) \\sim p_{\\theta}} [r(\\mathbf{s}, \\mathbf{a})])} \\end{aligned} \\end{equation} $$ In simpler way, as $T$ (the number of time steps) becomes very large, the average reward converges to the expected value of the reward given the policy $p_{\\theta}$.\nAlgorithm The reinforcement learning algorithms presented in this discussion, all of them would have more or less the same anatomy. The first part of our workflow is to generate samples, reinforcement learning is about learning through trial and error. Simply speaking, samples are the paths we have taken through the environment to reach our current position. Imagine that we have collected some samples. The next step is to find a suitable model for the dynamics in our model-based reinforcement learning algorithm. Then we turn into the final part, which is where you actually change your policy to make it better.\n$$ \\mathcal{J}(\\theta) = E_{\\pi}\\left[ \\sum_{t}r_{t} \\right] \\approx \\frac{1}{N}\\sum^{N}_{i = 1}\\sum_{t}r_{t}^{i} $$ $$ \\theta \\leftarrow \\theta +\\alpha \\nabla_{\\theta}\\mathcal{J}(\\theta) $$\ncomming soon\u0026hellip;\nValue functions comming soon\u0026hellip;\n","permalink":"https://wowking2018.github.io/posts/2024-02-28-lec1_rl_introduction/","summary":"Overview of Reinforcement Learning Reinforcement Learning (RL) is the art of decision making. It is about an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. There have been a number of advances in deep learning over the last decade that enhanced the robustness and effectiveness of reinforcement learning. Reinforcement learning nowadays has been applied in various domains, including robotics, game playing, autonomous vehicles, and more.","title":"Introduction to reinforcement learning"},{"content":"Introduction Physics Bowl 高中物理竞赛由美国物理教师协会 AAPT 主办，试题由协会注册的大学物理教授和教学经验丰富的高中物理老师组成理事会进行出题和评审.\n物理碗的发明者是蒂姆·英格兹比 (Tim Ingoldsby) 和比尔·阿诺德 (Bill Arnold)，他们在1980年提出了一项物理竞赛的提案和指导方针，后来发展成为现在的物理碗.\nHow to solve the equation? 那么现在我们来看生活大爆炸中的这一幕，第一季第13集，Sheldon和Leonard，Rajesh，Howard 还有 Leslie 分为了两队进行对抗.\n其中带箭头的直线为电子，波浪线为光子，这个图表示的是电子 ($e^{-}$) 和负缪子 ($\\mu$) 之间的弹性散射过程. 这个过程是通过交换一个虚光子 ($\\gamma$) 实现的，所以图中有一条波浪线. 式子中的各个符号表示了初末态的动量 $p$ 和自旋 $s$，以及光子的动量 $q$. 这个式子计算的是散射矩阵（S-matrix）的矩阵元，表示散射过程的概率幅1.\n这是QED的基本费曼图之一，其中Leonard，Rajesh，Howard的反应看上去并不知道这个题目这是极为不合理的，而且以剧中Sheldon的能力肯定能够解决这个问题. 不管怎样，看门人 (janitor) 回答说正确的答案是 $-8 \\pi \\alpha$. 以我个人所收集到的资料，至少在没有附加条件的情况下是无法得到这个答案的2，题目中给出的积分是\n$$ \\begin{equation} \\begin{aligned} (2\\pi)^4\\int[\\overline{\\upsilon}^{(s_3)}(p_3)(i\\sqrt{4\\pi\\alpha}\\gamma^{\\mu})\\upsilon^{(s_1)}(p_1)]\\frac{ig_{\\mu\\nu}}{q^{2}}[\\overline{\\upsilon}^{(s_4)}(p_4)(i\\sqrt{4\\pi\\alpha}\\gamma^{\\mu})\\upsilon^{(s_2)}(p_2)] \\\\ x\\delta^{(4)}(p_1-p_3-q)\\delta^{(4)}(p_2+q-p_4)\\mathrm{d}^4q \\end{aligned} \\end{equation} $$\n后续的过程中我会保持与题目中一致的记号进行推导. 首先，这并不是一个方程，台词中所说的 “Solve the equation” 显然是不对的，但为了保持统一我们暂时默认用积分式来代指这个式子. 同时对这个式子作相应的修改，去掉了 $x$，同时如果是负缪子应该用 $u$ 来表示\n$$ \\begin{equation} \\begin{aligned} (2\\pi)^4\\int[\\overline{\\upsilon}^{(s_3)}(p_3)(i\\sqrt{4\\pi\\alpha}\\gamma^{\\mu})\\upsilon^{(s_1)}(p_1)]\\frac{ig_{\\mu\\nu}}{q^{2}}[\\overline{u}^{(s_4)}(p_4)(i\\sqrt{4\\pi\\alpha}\\gamma^{\\nu})u^{(s_2)}(p_2)] \\\\ \\delta^{(4)}(p_1-p_3-q)\\delta^{(4)}(p_2+q-p_4)\\mathrm{d}^4q \\end{aligned} \\label{eq2} \\end{equation} $$\n根据狄拉克函数的计算规则有 $q = p_1- p_3 = p_4- p_2$时，对于 Eq. \\eqref{eq2} 中其余项进行合并可以得到\n$$ \\int \\delta^{(4)}(p_1-p_3-q)\\delta^{(4)}(p_2+q-p_4)\\mathrm{d}^4q = 1. $$\n积分式可以转化为\n$$ \\begin{equation} i{\\cal M}=\\frac{-i4\\pi\\alpha}{q^{2}}g_{\\mu\\nu}\\left[\\bar{\\upsilon}^{(s_3)}\\left(p_{3}\\right)\\gamma^{\\mu}\\upsilon^{(s_1)}\\left(p_{1}\\right)\\right]\\cdot[\\bar{u}^{(s_4)}\\left(p_{4}\\right)\\gamma^{\\nu}u^{(s_2)}\\left(p_{2}\\right)]. \\label{eq3} \\end{equation} $$\n根据动量守恒可以得到 $(p_1 - p_3)^2 = 2p^2 (1-\\cos \\theta)$，$\\theta$ 是散射粒子的夹角，对于中括号内的项可以表示为\n$$ \\begin{equation} \\bar{\\upsilon}(p^{\\prime})\\gamma^{\\mu}\\upsilon(p)=\\left(m_e-E_e-\\frac{p^2\\cos\\theta}{m_e+E_e},p\\sin\\theta,ip\\sin\\theta,p(1-\\cos\\theta)\\right)\n\\end{equation} $$\n其中， $m_{e}$ 是电子质量，$E_{e}$ 是对应能量，$p, p^{\\prime}$ 是散射进入和散射弹出的动量. 如果假设粒子是 ultra-relativistic，即 $E_{e}\\approx E_{\\mu}\\gg m_{\\mu}$，上式可以表示为(两项同理)\n$$ \\begin{equation} \\bar{\\upsilon}(p^{\\prime})\\gamma^{\\mu}\\upsilon(p)\\rightarrow p(1-\\cos\\theta,\\sin\\theta,i\\sin\\theta,1-\\cos\\theta) \\end{equation} $$\n故可得到 Eq. \\eqref{eq3} 中的项为\n$$ \\begin{equation} g_{\\mu\\nu}\\left[\\bar{v}^{(s_{3})}\\left(p_{3}\\right)\\gamma^{\\mu}v^{(s_{1})}\\left(p_{1}\\right)\\right]\\cdot\\left[\\bar{u}^{(s_{4})}\\left(p_{4}\\right)\\gamma^{\\nu}u^{(s_{2})}\\left(p_{2}\\right)\\right] \\approx 2 p^2 (1-\\cos \\theta)^2 \\end{equation} \\label{eq6} $$\n其结果并不影响我们计算散射矩阵元，将 Eq. \\eqref{eq6} 代入散射矩阵元，可以得到\n$$ \\begin{equation} \\mathcal{M} = - 4 \\pi \\alpha (1-\\cos \\theta). \\end{equation} $$\n所以如果图中问题是计算完全散射矩阵元，即 $\\theta = \\pi$，那么结果就是管理员所说的\n$$ \\begin{equation} \\mathcal{M} = -8\\pi \\alpha. \\end{equation} $$\n这个题目其实并不是很难，只要掌握了量子场论中的费曼规则，就可以根据图形把式子写出来. 但是在剧中，谢尔顿和他的队友们都没有回答出来，而是被一个门卫大叔轻松地说出了正确答案. 这不过是剧组的安排罢了，说多了都是剧情需要！\nReference 生活大爆炸第一季13集物理碗最后一题到底有多难？\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTBBT Physics Bowl [ANALYSIS OF THE LAST QUESTION]\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://wowking2018.github.io/posts/2023-07-30-the-big-bang-theory-physics-bowl-analysis/","summary":"Introduction Physics Bowl 高中物理竞赛由美国物理教师协会 AAPT 主办，试题由协会注册的大学物理教授和教学经验丰富的高中物理老师组成理事会进行出题和评审. 物理碗的发明者","title":"The Big Bang Theory-Physics Bowl Analysis"},{"content":"背景介绍 孤子的发现应追溯到1834年的夏日，英国科学家 J.S.Russel 骑马正沿着一条运河岸道旅行，偶然发现在狭窄的河床中行走的船突然停止前进，被船体带动的水团积聚在船头周围并剧烈地翻动着. 不久，一个圆形且轮廓分明的巨大孤立波峰开始形成，并急速离开船头向前运动. 波长约 10 米，高约 0.5 米，在行进中波的形状和速度并无明显变化，以后高度逐渐下降，在跟踪至三公里后，终于消失在蜿蜒的河道上. 这次发现的奇特景观促使 Russel 开始广泛的水波实验研究. 他称这沿着狭窄通道传播的、保持形状和速度不变的水包为“伟大的孤立波”，并意识到这是流体力学中某方程的稳定解. 然而, 由于当时数学水平与计算技术的限制，罗素的研究没有取得满意的进展，直到1895年，荷兰著名数学家 Korteweg 和他的学生 de Vries 在对孤波进行全面分析后指出这种波可近似为小振幅的长波，并以此建立了浅水波运动方程1：\n$$ \\begin{equation} \\frac{\\partial \\eta}{\\partial \\tau}=\\sqrt{\\frac{g}{h}} \\frac{\\partial}{\\partial \\xi}\\left(\\frac{3}{4} \\eta^2+\\alpha \\eta+\\frac{\\sigma}{2} \\frac{\\partial^2 \\eta}{\\partial \\xi^2}\\right), \\label{eq_1} \\end{equation} $$\n其中, $\\sigma=\\frac{1}{3} h^3-\\frac{T h}{\\rho g}, \\eta$ 为波面高度, $h$ 为水深, $g$ 为重力加速度, $\\rho$ 是水的密度, $\\alpha$ 是与水的匀速流动有关的小常数, $T$ 是水的表面张力. 此后 Korteweg 和 de Vries 利用行波法求出与 Russel 描述一致的孤波解, 争论才告终止.\n如果作以下变换\n$$ \\begin{equation} t = \\frac{1}{2} \\sqrt{\\frac{g}{h \\sigma}}\\tau, \\quad x = -\\frac{\\xi}{\\sqrt{\\sigma}}, \\quad u=\\frac{1}{2}\\eta +\\frac{1}{3}\\alpha \\end{equation} $$\n则 Eq. $\\eqref{eq_1}$ 可写成标准的形式\n$$ \\begin{equation} u_t+u_{x x x}+6 u u_x=0 \\label{eq_3} \\end{equation} $$\n后人为了纪念这两位伟大的学者对孤波作出的贡献将 Eq. $\\eqref{eq_1}$ 或 Eq. $\\eqref{eq_3}$ 称为 KdV 方程.\nKdV 方程的推导 1968年，Lax 曾提出了一种用线性算子导出孤子方程的方法 2, 具体计算方法如下. 首先线性算子 $\\mathcal{L}$ 满足\n$$ \\begin{equation} \\mathcal{L} \\phi=\\lambda \\phi . \\label{eq_4} \\end{equation} $$\n其中 $\\lambda$ 为谱参数, 如果只考虑等谱情况 ( $\\lambda$ 与时间无关), 即 $\\lambda_t=0$，其次 $\\phi$ 还满足\n$$ \\begin{equation} \\phi_t=\\mathcal{A} \\phi, \\label{eq_5} \\end{equation} $$\n其中 $\\mathcal{A}$ 也是线性算子. 将 Eq. $\\eqref{eq_4}$ 对 $t$ 求导, 同时结合 Eq. $\\eqref{eq_5}$ 有:\n$$ \\begin{equation} \\mathcal{L}_t \\phi+\\mathcal{L}\\mathcal{A} \\phi=\\mathcal{A}\\mathcal{L} \\phi \\end{equation} $$\n从而有\n$$ \\begin{equation} \\mathcal{L}_t=\\mathcal{A}\\mathcal{L} - \\mathcal{L}\\mathcal{A} = [\\mathcal{A}, \\mathcal{L}] . \\end{equation} $$\n这便是著名的 Lax 方程, 其中 $\\mathcal{L}, \\mathcal{A}$ 称为 Lax 对. 这里给出一种由 Lax 对进行推导得出 KdV 方程的较为简单的方式， 取 $\\mathcal{L}$ 为 Hamilton 算子 $\\mathcal{L}=\\partial^2 _{x} - u(x, t)$ (实际上Lax对确实和薛定谔方程是有联系的)， $\\mathcal{A}$ 为反对称算子 $\\mathcal{A}=\\alpha \\partial_x^3 + B(x,t) \\partial_x+ C(x,t) $ ， 其中 $\\alpha$ 是常数， $B(x, t), C(x, t)$ 是待定的项, 则\n$$ \\begin{equation} \\begin{aligned} \\mathcal{A}\\mathcal{L} \u0026 =\\left(\\alpha \\partial_x^3 + B(x,t) \\partial_x+ C(x,t) \\right)\\left(\\partial^2 _x - u\\right) \\\\ \u0026 = \\alpha \\partial_x^5 - \\alpha(u_{xxx}+3u_{xx}\\partial_{x} + 3u_{x}\\partial_{x}^2 + u\\partial_{x}^3) \\\\ \u0026 + B(\\partial_{x}^3-u_{x}-u\\partial_{x}) + C(\\partial_{x}^2-u)\\\\ \\mathcal{L}\\mathcal{A} \u0026 =\\left(\\partial^2 _x - u\\right)\\left(\\alpha \\partial_x^3 + B(x,t) \\partial_x+ C(x,t) \\right) \\\\ \u0026 = \\alpha \\partial_x^5 +(B_{xx}\\partial_{x} + 2B_{x}\\partial_{x}^2 + B \\partial_{x}^3) + (C_{xx}+2C_{x}\\partial_{x} + C\\partial_{x}^2) \\\\ \u0026 - u\\left(\\alpha \\partial_x^3 + B \\partial_x+ C \\right) \\end{aligned} \\end{equation} $$ 由这两个算子便有\n$$ \\begin{equation} \\begin{aligned} u _{t} \u0026 =\\mathcal{A}\\mathcal{L} -\\mathcal{L}\\mathcal{A} \\\\ \u0026 = -(3 \\alpha u_{x}+2B_{x})\\partial_{x}^2 -(3\\alpha u_{xx} + B_{xx}+2C_{x})\\partial_{x} +\\alpha u_{xxx} - Bu_{x} -C_{xx}. \\end{aligned} \\end{equation} $$ 其中等式右边有关 $\\partial_{x}$ 的项都为 $0$，可得\n$$ \\begin{equation} \\begin{aligned} \u0026amp;3 \\alpha u_{x}+2B_{x} = 0 \\ \u0026amp;3\\alpha u_{xx}+B_{xx}+2C_{x} = 0 \\end{aligned} \\end{equation} $$\n进行积分后可以得到\n$$ \\begin{equation} \\begin{aligned} \u0026amp;B = -\\frac{3}{2} \\alpha u \\ \u0026amp;C = -\\frac{3}{2} \\alpha u_{x}+\\frac{1}{2} B_{x} = -\\frac{3}{4}\\alpha u_{x} \\end{aligned} \\end{equation} $$\n可以得到 $u_{t} = \\alpha u_{xxx} - Bu_{x} -C_{xx}= \\frac{1}{4}\\alpha u_{xxx}+\\frac{3}{2}\\alpha u u_{x}$， 取 $\\alpha=-4$, 即为 $u_t+u_{x x x}+6 u u_x=0$.\nKdV方程求解 双曲正切法 首先, 运用行波法将所求解的非线性偏微分方程转化为非线性常微分方程, 即令 $\\xi=c(x-v t)$, 则 $u(x, t)=U(\\xi)$ ，有\n$$ \\begin{equation} \\frac{\\partial}{\\partial t} \\rightarrow-c v \\frac{\\mathrm{d}}{\\mathrm{d} \\xi}, \\frac{\\partial}{\\partial x} \\rightarrow c \\frac{\\mathrm{d}}{\\mathrm{d} \\xi} \\end{equation} $$\n然后, 将常微分方程积分, 直到微分方程中至少有一项不含导数项且尽可能 使方程中导数项具有较低阶数为止，积分常数都取为零. 引入 $Y=\\tanh (\\xi)$ 作为新的独立变量, 相应的导数变为\n$$ \\begin{equation} \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{d} \\xi} \\rightarrow \u0026\\left(1-Y^2\\right) \\frac{\\mathrm{d}}{\\mathrm{d} Y} \\\\ \\frac{\\mathrm{d}^2}{\\mathrm{d} \\xi^2} \\rightarrow \u0026\\left(1-Y^2\\right)\\left[-2 Y \\frac{\\mathrm{d}}{\\mathrm{d} Y}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^2}{\\mathrm{d} Y^2}\\right] \\\\ \\frac{\\mathrm{d}^3}{\\mathrm{d} \\xi^3} \\rightarrow \u0026-2 Y\\left(1-Y^2\\right)\\left[-2 Y \\frac{\\mathrm{d}}{\\mathrm{d} Y}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^2}{\\mathrm{d} Y^2}\\right] \\\\ \u0026+\\left(1-Y^2\\right)\\left[-2\\frac{\\mathrm{d}}{\\mathrm{d} Y}-2 Y \\frac{\\mathrm{d}^2}{\\mathrm{d} Y^2}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^3}{\\mathrm{d} Y^3}\\right] \\end{aligned} \\end{equation} $$ 求解过程 对于广义 KdV 方程, 运用双曲正切法进行求解3. 现在, 我们通过此方法对 Eq. $\\eqref{eq_3}$ 中的形式进行求解. 通过 $\\frac{\\partial}{\\partial t} \\rightarrow-c v \\frac{\\mathrm{d}}{\\mathrm{d} \\xi}, \\frac{\\partial}{\\partial x} \\rightarrow c \\frac{\\mathrm{d}}{\\mathrm{d} \\xi}$ 变换为\n$$ \\begin{equation} -v U_{\\xi}+6 U U_{\\xi}+c^2 U_{\\xi \\xi \\xi}=0, \\end{equation} $$\n两边进行积分, 可以得到\n$$ \\begin{equation} -v U+3 U^2+c^2 U_{\\xi \\xi}=0 \\end{equation} $$\n引入 $Y=\\tanh (\\xi)$ 作为新的独立变量,方程相应的可以变化为\n$$ \\begin{equation} -v S(Y)+3 S^2(Y)+c^2\\left(1-Y^2\\right) \\left(-2 Y \\frac{\\mathrm{d} S(Y)}{\\mathrm{d} Y}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^2 S(Y)}{\\mathrm{d} Y^2}\\right)=0 \\label{eq_12} \\end{equation} $$\n对于 $S=\\sum_{m=0}^m a_m Y^m$, 参数 $M$ 是通过平衡第二项(非线性)的阶数来确定. 不妨设 $S(Y)=\\gamma-\\gamma Y^2$, 代入 Eq. $\\eqref{eq_12}$进行化简可以得到\n$$ \\begin{equation} -v+3\\left(\\gamma-\\gamma Y^2\\right)+c^2 \\left(-2 Y \\frac{\\mathrm{d}\\left(1-Y^2\\right)}{\\mathrm{d} Y}+\\left(1-Y^2\\right) \\frac{\\mathrm{d}^2\\left(1-Y^2\\right)}{\\mathrm{d} Y^2}\\right)=0 \\end{equation} $$\n比较 $Y^0$ 和 $Y^2$ 的系数:\n$$ \\begin{equation} \\begin{gathered} -v+3 \\gamma-2 c^2=0 \\ -3 \\gamma+4 c^2+2 c^2=0 \\end{gathered} \\end{equation} $$\n现有三个末知数 $(v, \\gamma, c)$ 和两个方程, 所以可以选择 $c$ 作为自由参数. 发现其它变量为 $\\gamma=2 c^2, v=4 c^2$.\n最后, KdV 方程的孤波解为 3 :\n$$ \\begin{equation}\\begin{gathered} u(x,t) =2c^2\\left\\{1-\\tanh^2\\left[c\\left(x-4c^2t\\right)\\right]\\right\\} \\\\ =2c^2\\sec h^2\\left[c\\left(x-4c^2t\\right)\\right] \\end{gathered}\\end{equation} $$ P.S. 其中用 Lax pair 推导的方式几经修改，修正了一些细节内容.\n参考资料 Korteweg, Diederik Johannes, and Gustav De Vries. \u0026ldquo;On the change of form of long waves advancing in a rectangular canal, and on a new type of long stationary waves.\u0026rdquo; The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 39.240 (1895): 422-443.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLax, Peter D. \u0026ldquo;Integrals of nonlinear equations of evolution and solitary waves.\u0026rdquo; Communications on pure and applied mathematics 21.5 (1968): 467-490.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMalfliet, Willy. \u0026ldquo;Solitary wave solutions of nonlinear wave equations.\u0026rdquo; American journal of physics 60.7 (1992): 650-654.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://wowking2018.github.io/posts/2023-07-09-the-kdv-equation/","summary":"背景介绍 孤子的发现应追溯到1834年的夏日，英国科学家 J.S.Russel 骑马正沿着一条运河岸道旅行，偶然发现在狭窄的河床中行走的船突然停止前进，被船体带动的","title":"KdV 方程求解及其背景"},{"content":"从霜花到海岸线，自然界中分形无处不在，近现代物理学中同样不断能够找到分形的影子. 本文回顾了通过类比 Weierstrass 函数，可以从量子气体中寻求到相应的动力学分形. 随后，主要详细讲解了一种有趣的量子分形现象 —Hofstadter蝴蝶. 对于二维晶格的体系，加上对应的磁通，提取系统本征能量后便可以得到这种美丽的分形图案，最后举例了一些最新的相关的研究情况. (P.S. 这篇文章是我的课程设计，再次回看的时候添加了一些新内容，对以前的东西也有了新的理解.)\n引言 借用一句古语——“一花一世界，一树一菩提”，说的是从细微之处洞察宏观的哲学思考，而“一即是全，全即是一”，这是对分形最传神的表达. 每当看到那一张张分形图案时，不免思考这样美丽的图案是如何生成的，“分形”一出现便引起了年幼时我的兴趣. 现如今，随着电脑技术的兴起，分形被广泛运用到复杂图像的产生和处理上，其中包括大量电影里的星球表面，山川起伏的画面.\n1861年，德国数学家魏尔施特拉斯（Karl Theodor Wilhelm Weierstrass, 1815-1897) 发现了一个函数：\n$$ \\begin{equation} f(x)=\\sum_{n=0}^{\\infty} a^{n} \\cos \\left(b^{n} \\pi x\\right) \\end{equation} $$\n其中 $0\u0026lt;a\u0026lt;1$，$b$为正奇数，且需要满足 $ab\u0026gt;1+\\frac{3}{2}\\pi$，这个函数以及它处处连续而又处处不可导的证明首次出现在魏尔施特拉斯于1872年7月18日在普鲁士科学院出版的一篇论文中.\n从现在的数学角度看，毫无疑问这是一条分形曲线，然而“分形”这个概念直到一个世纪之后的1975年才由 Mandelbrot 提出来1. 在这里尝试一个简单的画图，Weierstrass函数有无穷项，但是随着 $n \\to \\infty, a^{n} \\cos \\left(b^{n} \\pi x\\right)\\to 0$，故可用有限项作为代替进行处理.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 x=linspace(-2,2,1000); S1=outputweier(0.5,1); S2=outputweier(0.5,2); S3=outputweier(0.5,3); subplot(3,1,1) plot(x,S1); subplot(3,1,2) plot(x,S2); subplot(3,1,3) plot(x,S3); xlabel(\u0026#39;\\it{x}\u0026#39;); %% function [S]=outputweier(a,b)%求和函数 syms f n x=linspace(-2,2,1000); f=a^n*cos(b^n*pi*x); S=symsum(f,n,0,100); end\t量子气体中的动力学分形 能量标度 上述的函数我们看起来如此奇怪，但却可以把它用在量子气体中. 类比于传统意义上的离散标度对称性 $W(b x) \\simeq a^{-1} W(x)$ (简而言之就是缩放后的自相似，discrete scaling symmetry)，在谐振二体相互作用的三粒子系统中，也有类似的 2：\n$$ \\begin{equation} E_{n+1} \\simeq \\lambda^{2} E_{n} \\end{equation} $$\n利用能量标度关系，构造 Loschmidt 振幅 $\\mathcal{L}(t)$. 受“分形”函数的启示，与量子系统的相似性启发可以构造出相应的类 Weierstrass 函数：\n$$ \\begin{equation} \\mathcal{L}(t) \\propto \\sum_{n=0}^{N} \\lambda^{-n D} e^{-(i / \\hbar) \\lambda^{2 n} E_{0} t} \\end{equation} $$\nLoschmidt 振幅“分形”图像 通过类比的方法，选择初始波函数、测量、维度、适当的能量和时间尺度的要求，便可以用超冷量子气体来实现在时域上表现出分形行为2.\n数值模拟表明，所有这些要求可以很容易地同时满足实际参数在冷原子气体.目前的计算是基于单个粒子图，忽略了粒子间的相互作用.\n霍夫斯塔特蝴蝶(Hofstadter\u0026rsquo;s Butterfly) 1976年，Douglas Hofstadter 在研究 Bloch 电子在外磁场作用下的量子现象时发现电子的能量和磁场强度可形成分形图像.\n朗道能级 事实上，从现在的眼光来看，Hofstadter\u0026rsquo;s Butterfly本身就是朗道能级的二维格点版本. 带电粒子能量在一系列分立的数值中取值，形成朗道能级. 求解外加磁场下的薛定谔方程，从而得到电子能级的分布，即为朗道能级. 外加磁场为均匀磁场，沿 $z$ 方向，即 $\\textbf{B}=(0,0,B)$.\n由 $\\mathbf{B}=\\nabla \\times \\hat{\\mathbf{A}}$，取朗道规范可得磁矢势 $\\hat{\\mathbf{A}}=(-B y , 0, 0)$，式中 $\\lvert {B} \\rvert$，这样的规范下电子的正则动量为 $\\hat{\\textbf{p}}+q \\hat{\\textbf{A}}$，电子的哈密顿量可以写为：\n$$ \\begin{equation} \\hat{H}=\\frac{(\\hat{p}+q \\hat{A})^{2}}{2 m}=\\frac{1}{2 m}\\left[\\left(\\hat{p_{x}}-q B y\\right)^{2}+\\hat{p_{y}}^{2}+\\hat{p_{z}^{2}}\\right] \\end{equation} $$\n$\\hat{H}$ 不显含 $x,z$，则 $\\hat{H}$ 与 $\\hat{p_x},\\hat{p_z}$ 对易，可选取 $(\\hat{H},\\hat{p_x},\\hat{p_z})$ 力学量完全集，共同本征函数形式可得为 $\\psi(x, y, z)=e^{\\frac{i}{\\hbar}\\left(p_{x} x+p_{z} z\\right)} \\phi(y)$，代入 $\\hat{H} \\psi=E \\psi$，并做变量替换 $\\xi=y+\\frac{p_{x}}{q B},\\omega=\\frac{q B}{m},E^{\\prime}=E-\\frac{p_{z}^{2}}{2 m}$，得到如下方程：\n$$ \\begin{equation} \\left[-\\frac{\\hbar^{2}}{2 m} \\frac{\\mathrm{d}^{2}}{\\mathrm{d} \\xi^{2}}+\\frac{1}{2} m \\omega^{2} \\xi^{2}\\right] \\phi(\\xi)=E^{\\prime} \\phi(\\xi) \\end{equation} $$\n该方程在形式上与一维谐振子势的定态方程完全一致，求解步骤相同，可以得到能级的分立解为：\n$$ \\begin{equation} E_{n}^{\\prime}=\\left(n+\\frac{1}{2} \\hbar \\omega\\right), n \\in N \\end{equation} $$\n量子霍尔效应与Hofstadter蝴蝶的联系 所谓量子霍尔效应与经典理论有何不同呢？在经典理论下，自由电子在电场 $\\mathbf{E}$ 和磁场 $\\mathbf{B}$ 的作用下的运动方程可写为：\n$$ \\begin{equation} m\\left(\\frac{\\mathrm{d}}{\\mathrm{d} t}+\\frac{1}{\\tau}\\right) \\vec{v}=-e(\\vec{E}+\\vec{v} \\times \\vec{B}) \\end{equation} $$\n其中 $\\tau$ 为电子的弛豫时间，$\\frac{m \\vec{v}}{\\tau}$ 这一项表示电子运动过程中的碰撞效应，在一个二维系统中写为分量形式，当到达稳态时速度不随时间变化，已知电流密度 $J=n e v$，并令$\\sigma=\\lvert e \\rvert n v, v=\\lvert e \\rvert \\tau/ m$，写成矩阵形式为：\n$$ \\begin{equation} \\left[\\begin{array}{l} J_{x} \\\\ J_{y} \\end{array}\\right]=\\sigma\\left[\\begin{array}{cc} 1 \u0026 -\\mu B \\\\ \\mu B \u0026 1 \\end{array}\\right]^{-1}\\left[\\begin{array}{c} E_{x} \\\\ E_{y} \\end{array}\\right] \\end{equation} $$ 当施加磁场后会使得矩阵出现了非对角元. 写成电阻形式 $E=\\rho_{2 \\times 2} J$，可以得到经典霍尔效应的电阻率：\n$$ \\begin{equation} \\rho_{x x}=\\sigma^{-1}, \\rho_{y x}=-\\rho_{x y}=\\frac{B} {|e|} n \\end{equation} $$\n然而，1980年德国物理学家 Klaus von Klitzing 发现在极低温、强磁场条件下，二维电子气的霍尔电阻出现了平台化特征，其值等于量子化电阻值 $(h/e^2)$ 的整数倍. 在霍尔电阻表现为量子化平台的同时，纵向电阻则趋于零3. 在实际实验研究中，更常采用的是固定二维电子气的密度，在维持流过样品的电流恒定的条件下，测量霍尔电压；随外加磁场的变化，可见在低磁场下霍尔电阻的确与外磁场的磁感应强度成线性关系；但在高磁场区则得到霍尔电阻的平台状结构.\n横向霍尔电阻的平台结构与纵向电阻的SdH振荡\n1976年，Douglas Hofstadter在研究Bloch电子在外磁场作用下的量子现象时发现电子的能量和磁场强度可形成分形图像. 在他的博士论文中研究了晶格里跃迁几率幅为复数的电子行为，这种复跃迁破坏了时间空间反演对称，后续又引出了Chern绝缘体的话题. 如图所示，蝴蝶能谱的整数区域其实就是霍尔电导率的量子化台阶. 正是这种有趣的量子分形现象———Hofstadter蝴蝶.\n横轴为复跃迁的复角度，纵轴为费米能. 用整数量子标记间隙，用相同颜色的间隙表示具有相同霍尔电导率量子数的量子霍尔态. 4\n晶格中的“分形”状能谱 二维方格子 首先，考虑如一个二维晶体体系，每个格点(交叉点)上放一个各向同性的电子轨道能级. 体系近似的哈密顿量可写成：\n$$ \\begin{equation} H=\\sum_{m, n} H_{m, n}^{0}+\\sum_{m, n, m^{\\prime}, n^{\\prime}} \\frac{e^{2}}{4 \\pi \\varepsilon\\left|\\hat{\\vec{r}}_{m, n}-\\hat{\\vec{r}}_{m^{\\prime}, n^{\\prime}}\\right|} \\end{equation} $$ 每个格点上都有一个初始波函数 $H_{m, n}^{0} \\vert m, n\\rangle=\\mu \\vert m, n\\rangle$，且认为不同格点之间的波函数相互正交 $\\left\\langle m^{\\prime}, n^{\\prime} \\mid m, n\\right\\rangle=\\delta_{m^{\\prime}, m} \\delta_{n^{\\prime}, n}$. 直接可以计算哈密顿量矩阵，然后对角化哈密顿量，便得到了体系的本征能量以及本征波矢. 矩阵元为5：\n$$ \\begin{equation} \\left\\langle m, n|H| m^{\\prime}, n^{\\prime}\\right\\rangle=\\mu \\delta_{m, m^{\\prime}} \\delta_{n, n^{\\prime}}+\\left\\langle m, n\\left|\\frac{e^{2}}{4 \\pi \\varepsilon\\left|\\hat{\\vec{r}}_{m, n}-\\hat{\\vec{r}}_{m^{\\prime}, n^{\\prime}}\\right|}\\right| m^{\\prime}, n^{\\prime}\\right\\rangle \\end{equation} $$ 换用投影算符的形式来描述这样一个哈密顿量矩阵如下：\n$$ \\begin{equation} \\hat{H}=\\sum_{m} \\mu|m, n\\rangle\\langle m, n|+\\sum_{m, n ; m^{\\prime}, n^{\\prime}}{}^{\\prime}\\left(t_{m n, m^{\\prime} n^{\\prime}}|m, n\\rangle\\left\\langle m^{\\prime}, n^{\\prime}\\right|+h . c\\right) \\label{eq_12} \\end{equation} $$\n其中，$t _{m n, m^{\\prime} n^{\\prime}} = \\left\\langle m, n \\lvert \\frac{e^{2}}{4 \\pi \\varepsilon \\lvert \\hat{\\vec{r}} _{m, n}-\\hat{\\vec{r}} _{m^{\\prime}, n^{\\prime}} \\rvert } \\rvert m^{\\prime}, n^{\\prime}\\right\\rangle$, Eq. $\\eqref{eq_12}$ 中 $\\sum{}^{\\prime}$ 表示 $m \\neq m^{\\prime}; n \\neq n^{\\prime}$. 考虑到不同格点之间的波函数交叠很小，一般只考虑最近邻格点之间的交叠，认为处理最近邻的相互作用，由此哈密顿量可写成：\n$$ \\begin{equation} \\hat{H}=\\sum_{m} \\mu|m, n\\rangle\\langle m, n|+\\sum_{m, n}t_{x}|m+1, n\\rangle\\left\\langle m, n\\left|+t_{y}\\right| m, n+1\\right\\rangle\\langle m, n|+h . c \\end{equation} $$\n在数值算的时候，假设格点上没有初始波函数，相应的哈密顿量取其一部分为，考虑临近相互作用的形式用直积(方便计算)可以写为\n$$ \\begin{equation} H=\\sum_{m, n} t_{x}|m\\rangle\\langle m+1|\\otimes| n\\rangle\\left\\langle n\\left|+t_{y}\\right| m\\right\\rangle\\langle m|\\otimes| n\\rangle\\langle n+1|+h.c \\end{equation} $$\n加静磁场后的方格子 当静磁场对该晶格系统产生影响时，情况又会发生不一样的变化. 电磁场中自由电子的哈密顿量代入薛定谔方程后可以写为6：\n$$ \\begin{equation} \\left[\\frac{(\\hat{\\vec{p}}+e \\vec{A})^{2}}{2 m}-e \\varphi\\right] \\psi=i \\hbar \\frac{\\partial}{\\partial t} \\psi \\end{equation} $$\n如果我们把波函数写成$\\psi^{\\prime}(\\vec{r})=e^{-i \\frac{e}{\\hbar} \\int_{r_{0}}^{\\vec{r}} \\vec{A}\\left(\\vec{r}^{\\prime}\\right) d \\vec{r}^{\\prime}} \\psi(\\vec{r})$，如果对它做一个空间平移，由于磁场的存在，这样的一个空间平移会给波函数带来一个额外的相位. 然后让每个格子都有一个$\\phi$的磁通时，把哈密顿量写成矩阵的形式后就可以直接算本征值，提取系统的本征能量，便可以绘制出Hofstadter蝴蝶.\nSchematic of the experiments by Aidelsburger et al.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 clear all; close all; % Set up simulation parameters hop_energy = 1; num_sites = 10; num_phases = 200; site_indices = 1:num_sites; phase_values = linspace(0, 2, num_phases); % Initialize energy array energy_spectrum = zeros(num_phases, num_sites^2); % Compute energy spectrum for phase_index = 1:num_phases phase = phase_values(phase_index) * pi; hop_matrix = diag(ones(1, num_sites - 1), 1) + diag(ones(1, num_sites - 1), -1); phase_matrix = diag(exp(1j * phase * site_indices)); hamiltonian = kron(eye(num_sites), hop_matrix * hop_energy) + ... kron(diag(ones(1, num_sites - 1), 1), phase_matrix) + ... kron(diag(ones(1, num_sites - 1), -1), phase_matrix\u0026#39;); [~, eigvals] = eig(hamiltonian); energy_spectrum(phase_index, :) = diag(eigvals); end % Plot energy spectrum figure(); hold on; for band_index = 1:num_sites^2 band_energies = energy_spectrum(:, band_index); band_energies(end) = NaN; colors = band_energies; patch(phase_values / 2, band_energies, colors, \u0026#39;edgecolor\u0026#39;, \u0026#39;flat\u0026#39;, \u0026#39;facecolor\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;Linewidth\u0026#39;, 1); end hold off; box on; colormap(hsv); axis square; xlim([0, 1]); ylim([-4, 4]); 正方形晶格有限截面的能谱数值模拟图：(a)$2×2$网格，(b)$3×3$网格，(c)$4×4$网格，(d)$6×6$网格，(e)$7×7$网格, (f)$10× 10$网格\n特别值得一提的是,如果对实空间上离散的Schródinger方程进行傅里叶变换，得到如下方程：\n$$ \\begin{equation} 2 \\cos \\left(k_{1} a+\\frac{2 \\pi p r}{q}\\right) \\tilde{\\psi}_{r}(\\mathbf{k})+e^{i k_{2} a} \\tilde{\\psi}_{r+1}(\\mathbf{k})+e^{-i k_{2} a} \\tilde{\\psi}_{r-1}(\\mathbf{k})=-\\frac{E(\\mathbf{k})}{t} \\tilde{\\psi}_{r}(\\mathbf{k}) \\label{eq_16} \\end{equation} $$ 其中 $\\mathbf{k}=\\left(k_{1}, k_{2}\\right)$，Eq. $\\eqref{eq_16}$就是Harper方程. 用数值方法求解时，如果我们不断改变 $\\Phi / \\Phi_{0}$ (即朗道能级简并度)时，结果也可以得到这样一个美丽的分形结构.\n朗道扇形能谱 对于无限二维方形晶格的紧束缚模型，具有能量色散关系：\n$$ \\begin{equation} E(\\mathbf{k})=-2|t|\\left(\\cos \\left(k_{x} \\alpha \\right)+\\cos \\left(k_{y} \\alpha \\right)\\right) \\label{eq_17} \\end{equation} $$\n由于能量范围是 $-4 \\lvert t \\rvert \u0026lt; E \u0026lt; 4 \\lvert t \\rvert$，带宽为 $8\\lvert t \\rvert$，在二维紧束缚带的底部附近由Eq. $\\eqref{eq_17}$ 描述，能量近似为类自由电子，约化质量为7：\n$$ \\begin{equation} E(\\mathbf{k}) \\approx-4|t|+\\frac{\\hbar^{2}}{2 m^{*}}\\left(k_{x}^{2}+k_{y}^{2}\\right) \\end{equation} $$\n其中，有效质量为 $m^{\\ast}=\\hbar^{2} /\\left(2 \\lvert t \\rvert \\alpha^{2}\\right)$，回旋频率可以写为 $\\omega_{c}=\\frac{e B}{m^{\\ast}}=\\frac{4 \\pi \\alpha \\lvert t \\rvert}{\\hbar}$，因此，靠近谱带底部的朗道能级可以写成\n$$ \\begin{equation} \\frac{E}{|t|}=-4+4 \\pi \\alpha\\left(n+\\frac{1}{2}\\right) \\label{eq_19} \\end{equation} $$\n(a)$10× 10$方阵区的能谱;(b)朗道扇形能谱. 对于半满带的情况，化学势$\\mu$设为零.\n根据图(b)所示,即为Eq. $\\eqref{eq_19}$给出的朗道水平扇面. 正是这种朗道能级的扇形引起了德哈斯-范阿尔芬效应(De Haas-van Alphen effect,纯金属晶体的磁化强度随外加磁场的增加而发生振荡的现象)，在这些效应占据的朗道能级中，化学势 $\\mu$ 和费米能量 $E_p$ 下降到下一个朗道能级. 同样，有限系统成键轨道的能量在稠密区域增加，在稀疏区域迅速下降，这些轨道在二维系统中表现出磁振荡的特征.\n实验相关 在凝聚态物理学中，Hofstadter蝴蝶理论描述了晶格中磁场中不相互作用的二维电子的光谱特性，并在整数量子霍尔效应理论和拓扑量子数理论中起着重要的作用，但由于该现象需要苛刻的实验条件，直到20年后才在实验上观察到这样一幅美丽的景象.\n1997年，Hofstadter蝴蝶在由一组散射体装备的微波波导的实验中得以再现8. 正是由于散射体的微波波导的数学描述与磁场中的Bloch波之间的相似性，使得散射体周期序列的Hofstadter蝴蝶得以再现.\n(a)散射体周期性排列的透射光谱，图中上部是通过反射得到的可以看到前两个Bloch带，显示了Hofstadter蝴蝶的两个部分. (其中黑色和白色分别对应高透射率和低透射率); (b)四个Bloch纹，隐约可以看见每一个中的Hofstadter能谱.\n2017年9月，Google的John Martinis小组与CQT的Angelakis 小组合作发表了使用9个超导量子比特的相互作用光子，去模拟磁场中电子的2维结果9，复现出了Hofstadter蝴蝶.\n(a)在无量纲磁场 $b$ 的 $100$ 个取值范围为 $0-1$ 时，恢复了Hofstadter蝴蝶；(b)对于每个 $b$ 值，识别出 $9$ 个峰，并将它们的位置绘制为一系列彩点，点的颜色是测量特征值与数值计算特征值之差的绝对值.\n2020年，有研究小组报告了在六方氮化硼基底上制造的石墨烯器件中存在霍夫施塔特蝴蝶光谱的证据10. 在石墨烯晶格与氮化硼接近零角度，由外加磁场和大规模 moiré 结构之间的相互作用产生了蝴蝶型光谱.\n结语 从霜花到海岸线，自然界中分形无处不在，近现代物理学中同样不断能够找到分形的影子. 这一种有趣的量子分形现象——Hofstadter蝴蝶，从它与整数量子霍尔效应的联系出发，最近火热的转角石墨烯体系、分数陈绝缘体等与其相关密切. 从现在的眼光来看，Hofstadter\u0026rsquo;s Butterfly本身就是朗道能级的二维格点版本. 本文也讨论了对于二维晶格的体系，加上对应的磁通，提取系统本征能量后便可以得到一个美丽的分形图案. 但若要在实验上得以实现，需要极大的磁通，最后也举例了最新的实验观测情况. 实际上，Hofstadter蝴蝶中与之相关的物理现象有很多，例如分数量子霍尔效应也与之有重要的联系.\nReferences Mandelbrot, Benoit B. \u0026ldquo;Les objets fractals: forme, hasard et dimension.\u0026rdquo; (1975).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGao, Chao, Hui Zhai, and Zhe-Yu Shi. \u0026ldquo;Dynamical fractal in quantum gases with discrete scaling symmetry.\u0026rdquo; Physical Review Letters 122.23 (2019): 230402.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKlitzing, K. V., Gerhard Dorda, and Michael Pepper. \u0026ldquo;New method for high-accuracy determination of the fine-structure constant based on quantized Hall resistance.\u0026rdquo; Physical review letters 45.6 (1980): 494.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSatija, Indubala. \u0026ldquo;Pythagorean triplets, integral apollonians and the Hofstadter butterfly.\u0026rdquo; arXiv preprint arXiv:1802.04585 (2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJános K. Asbóth, László Oroszlány, András Pályi. \u0026ldquo;A Short Course on Topological Insulators: Band Structure and Edge States in One and Two Dimensions.\u0026rdquo; Springer International Publishing, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCage, Marvin E., et al. \u0026ldquo;The quantum Hall effect.\u0026rdquo; Springer Science \u0026amp; Business Media, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAnalytis, James G., Stephen J. Blundell, and Arzhang Ardavan. \u0026ldquo;Landau levels, molecular orbitals, and the Hofstadter butterfly in finite systems.\u0026rdquo; American Journal of Physics 72.5 (2004): 613-618.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKuhl, U., and H-J. Stöckmann. \u0026ldquo;Microwave realization of the Hofstadter butterfly.\u0026rdquo; Physical review letters 80.15 (1998): 3232.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRoushan, Pedram, et al. \u0026ldquo;Spectroscopic signatures of localization with interacting photons in superconducting qubits.\u0026rdquo; Science 358.6367 (2017): 1175-1179.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBarrier, Julien, et al. \u0026ldquo;Long-range ballistic transport of Brown-Zak fermions in graphene superlattices.\u0026rdquo; Nature Communications 11.1 (2020): 5756.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://wowking2018.github.io/posts/2023-07-01-hofstadter-butterfly/","summary":"从霜花到海岸线，自然界中分形无处不在，近现代物理学中同样不断能够找到分形的影子. 本文回顾了通过类比 Weierstrass 函数，可以从量子气体中寻求到相应的动力学","title":"量子分形世界-Hofstadter蝴蝶"},{"content":"RNN介绍 RNN(循环神经网络)是在自然语言处理NLP领域中最先被用起来的，比如，RNN可以为语言模型来建模. 循环神经网络种类繁多，最简单的基本循环神经网络可以被理解为隐藏层内循环，如下图所示：\n如果我们把上面的图展开，循环神经网络也可以画成下面这个样子：\n现在看上去就比较清楚了，这个网络在 $t$ 时刻接收到输入 $\\mathrm{x}_ {t}$ 之后，隐藏层的值是 $\\mathrm{s} _ {t}$ ，输出值是 $\\mathrm{o}_ {t}$. 关键一点是， $\\mathrm{s}_ {t}$ 的值不仅仅取决于 $\\mathrm{x}_ {t}$ , 还取决于 $\\mathrm{s}_ {t-1}$ . 我们可以用下面的公式来表示循环神经网络的计算方法:\n$$ \\begin{equation} \\begin{aligned} \u0026 \\mathrm{o}_t=g\\left(V \\mathrm{~s}_t\\right)\\\\ \u0026 \\left.\\mathrm{s}_t=f\\left(U \\mathrm{x}_t+W \\mathrm{~s}_{t-1}\\right) \\right. \\end{aligned} \\label{eq1} \\end{equation} $$ 输出层是一个全连接层，也就是它的每个节点都和隐 藏层的每个节点相连. $\\mathrm{V}$ 是输出层的权重矩阵， $g$ 是激活函数. 隐藏层是循环层. ${U}$ 是输入 $x$ 的权重矩阵， ${W}$ 是上一次的值 $\\mathrm{s}_ {t-1}$ 作为这一次的 输入的权重矩阵， $f$是激活函数.\n从 Eq. \\eqref{eq1} 中我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 $W$. 如果反复把隐藏层的计算公式带入到输出层，我们将得到:\n$$ \\begin{equation} \\begin{aligned} \\mathrm{o}_t \u0026 =g\\left(V \\mathrm{~s}_t\\right) \\\\ \u0026 =V f\\left(U \\mathrm{x}_t+W \\mathrm{~s}_{t-1}\\right) \\\\ \u0026 =V f\\left(U \\mathrm{x}_t+W f\\left(U \\mathrm{x}_{t-1}+W \\mathrm{~s}_{t-2}\\right)\\right) \\\\ \u0026 =V f\\left(U \\mathrm{x}_t+W f\\left(U \\mathrm{x}_{t-1}+W f\\left(U \\mathrm{x}_{t-2}+W \\mathrm{~s}_{t-3}\\right)\\right)\\right) \\\\ \u0026 =V f\\left(U \\mathrm{x}_t+W f\\left(U \\mathrm{x}_{t-1}+W f\\left(U \\mathrm{x}_{t-2}+W f\\left(U \\mathrm{x}_{t-3}+\\ldots\\right)\\right)\\right)\\right) \\end{aligned} \\end{equation} $$ 从上面可以看出，循环神经网络的输出值 $o_t$ ，是受前面历次输入值 $\\mathrm{x}_ t , \\mathrm{x}_ {t-1} , \\mathrm{x}_ {t-2} , \\mathrm{x}_ {t-3} , \\ldots$ 影响的. 有了以上的讲解，我们将很容易理解论文中的这一部分内容.\n$$ \\begin{equation} \\begin{gathered} \\boldsymbol{h}_t=\\sigma^{(h)}\\left(\\boldsymbol{W}^{(h)} \\cdot \\boldsymbol{h}_{t-1}+\\boldsymbol{W}^{(x)} \\cdot \\boldsymbol{x}_t\\right) \\\\ y_t=\\sigma^{(y)}\\left(\\boldsymbol{W}^{(y)} \\cdot \\boldsymbol{h}_t\\right) \\end{gathered} \\label{eq3} \\end{equation} $$ 引入波动方程与RNN相结合 波的标量分布为 $u(x, y, z)$ ，其含时演化过程可以由以下方程进行描述\n$$ \\begin{equation} \\frac{\\partial^2 u}{\\partial t^2}-c^2 \\cdot \\nabla^2 u=f \\end{equation} $$\n其中， $\\nabla^2=\\frac{\\partial^2}{\\partial x^2}+\\frac{\\partial^2}{\\partial y^2}+\\frac{\\partial^2}{\\partial z^2}$ 是Laplace算符, $c=c(x, y, z)$ 是空间中的波速且会随空间位置的不同发生变化， $f=f(x, y, z, t)$ 是原函数项. 对 Eq. 4 进行有限差分离散，时间步长为$\\Delta t$，得到递归关系方程\n$$ \\frac{u_{t+1}-2 u_t+u_{t-1}}{\\Delta t^2}-c^2 \\cdot \\nabla^2 u_t=f_t $$\n将以上的差分方程可以转化为\n$$ \\begin{equation} \\left[\\begin{array}{c} u_{t+1} \\\\ u_t \\end{array}\\right]=\\left[\\begin{array}{cc} 2+\\Delta t^2 \\cdot c^2 \\cdot \\nabla^2 \u0026 -1 \\\\ 1 \u0026 0 \\end{array}\\right] \\cdot\\left[\\begin{array}{c} u_t \\\\ u_{t-1} \\end{array}\\right]+\\Delta t^2 \\cdot\\left[\\begin{array}{c} f_t \\\\ 0 \\end{array}\\right] \\end{equation} $$ 结合 Eq. \\eqref{eq3} 我们很容易会得到相对应的形式\n$$ \\begin{equation} \\begin{aligned} \\boldsymbol{h}_t \u0026 =\\boldsymbol{A}\\left(\\boldsymbol{h}_{t-1}\\right) \\cdot \\boldsymbol{h}_{t-1}+\\boldsymbol{P}^{(\\mathrm{i})} \\cdot \\boldsymbol{x}_t \\\\ \\boldsymbol{y}_t \u0026 =\\left|\\boldsymbol{P}^{(\\mathrm{o})} \\cdot \\boldsymbol{h}_t\\right|^2 \\end{aligned} \\end{equation} $$ 方程中 $c=c(x, y, z)$ 部分对应于材料内的物理构型和布局. 与标准RNN类似，隐藏状态与波动方程输入和输出之间的联系由线性算子 $\\boldsymbol{P}^{(\\mathrm{i})}$ 和 $\\boldsymbol{P}^{(\\mathrm{o})}$ 定义. 现在我们可以对这两个线性算子进行详细的分析，根据文中作者的说法1\n$$ \\begin{equation} \\begin{aligned} \\boldsymbol{P}^{(\\mathrm{i})} \u0026 \\equiv\\left[\\begin{array}{c} \\boldsymbol{M}^{(\\mathrm{i})} \\\\ \\mathbf{o} \\end{array}\\right] \\\\ \\boldsymbol{P}^{(\\mathrm{o})} \u0026 \\equiv\\left[\\boldsymbol{M}^{(\\mathrm{o}) T}, \\mathbf{o}\\right] \\end{aligned} \\end{equation} $$ 其中，$\\mathrm{o}$ 是零矩阵，相对应的形式在于把输入向量 $\\boldsymbol{x}_ t$ 的输入写成矩阵向量的乘法形式：\n$$ \\Delta t^2 \\boldsymbol{f}_t\\equiv \\boldsymbol{M}^{(i)} \\cdot \\boldsymbol{x}_t $$\n特别注意，RNN在每个时间步的输出是由标量场的强度测量给出的，\n$$ \\boldsymbol{y}_ t=\\boldsymbol{M}^{(\\mathrm{o}) T} \\cdot \\boldsymbol{u}_t^2 $$\n以上便是一个简单的例子用来说明RNN与波动方程的联系.\n训练物理系统 该论文中演示了波动方程的动力学如何通过构造非均匀材料分布来训练来分类元音的分类，ae, ei, 和 iy. 图D中的黑色点源即为音源(类比为输入层)，通过中间浅色部分(隐藏层)传播，右边定义了三个探测点(输出层)，每个探测点分配给三个元音类中的一个，对相对应的结果进行反馈. 图C中的线即为三个元音的功率积分结果.\n为了方便数值结果演示，考虑由两种材料组成的二值化系统，归一化后的波速分别为 $c_0=1.0$ 和 $c_1=0.5$. 本文采用了反向优化的传播思路，Adam的方法对材料密度属性进行了优化，其优化结果被展示在下图D之中.\n引入一个吸收区域来近似一个对应的边界条件，对应于图B中的灰色区域. 此外，与传统的RNN不同，波动方程施加了能量守恒约束，防止隐态范数和输出信号的无限增长. 该区域由阻尼系数 $b(r, y)$ 定义，带阻尼的标量波动方程可以写为\n$$ \\begin{equation} \\frac{\\partial^2 u}{\\partial t^2}+2 b \\cdot \\frac{\\partial u}{\\partial t}=c^2 \\cdot \\nabla^2 u+f \\end{equation} $$\n其中，$u$ 为未知标量字段，$b$ 为阻尼系数. 在数学形式上，该论文对此简化为\n$$ \\begin{equation} b(u)=\\frac{b_0}{1+(\\frac{u}{u_\\mathrm{th}})^2}. \\end{equation} $$\n假设 $b$ 可以在空间上变化，但与频率无关. 对于以 $t$ 为索引的时间步长，用二阶微分的中心差分公式\n$$ \\begin{equation} \\frac{u_{t+1}-2 u_t+u_{t-1}}{\\Delta t^2}+2 b \\frac{u_{t+1}-u_{t-1}}{2 \\Delta t}=c^2 \\nabla^2 u_t+f_t \\end{equation} $$\n现在可以用 $u_{t+1}$ 形成一个递归关系，从而得到\n$$ \\begin{equation} \\begin{aligned} \u0026 \\left(\\frac{1}{\\Delta t^2}+\\frac{b}{\\Delta t}\\right) u_{t+1}-\\frac{2}{\\Delta t^2} u_t+\\left(\\frac{1}{\\Delta t^2}-\\frac{b}{\\Delta t}\\right) u_{t-1}=c^2 \\cdot \\nabla^2 u_t+f_t \\\\ \u0026 \\left(\\frac{1}{\\Delta t^2}+\\frac{b}{\\Delta t}\\right) u_{t+1}=\\frac{2}{\\Delta t^2} u_t-\\left(\\frac{1}{\\Delta t^2}-\\frac{b}{\\Delta t}\\right) u_{t-1}+c^2 \\cdot \\nabla^2 u_t+f_t \\\\ u_{t+1}= \u0026 \\left(\\frac{1}{\\Delta t^2}+\\frac{b}{\\Delta t}\\right)^{-1}\\left[\\frac{2}{\\Delta t^2} u_t-\\left(\\frac{1}{\\Delta t^2}-\\frac{b}{\\Delta t}\\right) u_{t-1}+c^2 \\cdot \\nabla^2 u_t+f_t\\right] \\end{aligned} \\end{equation} $$ 类比常规的波动方程形式，我们可以得到在论文中实际考虑阻尼系数项的形式：\n$$ \\begin{equation} \\left[\\begin{array}{c} u_{t+1} \\\\ u_t \\end{array}\\right]=\\left[\\begin{array}{cc} \\frac{2+\\Delta t^2 \\cdot c^2 \\cdot \\nabla^2}{1+\\Delta t \\cdot b} \u0026 \\frac{-1-\\Delta t \\cdot b}{1+\\Delta t \\cdot b} \\\\ 1 \u0026 0 \\end{array}\\right] \\cdot\\left[\\begin{array}{c} u_t \\\\ u_{t-1} \\end{array}\\right]+\\Delta t^2 \\cdot\\left[\\begin{array}{c} f_t \\\\ 0 \\end{array}\\right] \\end{equation} $$ 作者在文中提到在数值计算中，对于算符 $\\nabla^2$ 的处理采取了离散形式的处理\n$$ \\nabla^2 u_t=\\frac{1}{h^2}\\left[\\begin{array}{ccc} 0 \u0026 1 \u0026 0 \\\\ 1 \u0026 -4 \u0026 1 \\\\ 0 \u0026 1 \u0026 0 \\end{array}\\right] * u_t $$ 其中，$h$ 为空间网格的步长，同时$*$指的是卷积运算. 之所以采取了这样的一个形式，同样也是采用了二阶微分的近似. 首先引入这个刚刚讲过的二阶微分:\n$$ \\begin{equation} \\nabla^2 f=\\frac{\\partial^2 f}{\\partial x^2}+\\frac{\\partial^2 f}{\\partial y^2} \\end{equation} $$\n由联系二阶差分的思想:\n$$ \\begin{equation} \\begin{aligned} \u0026 \\frac{\\partial^2 f}{\\partial x^2}=f(x+1)-2f(x)+f(x-1) \\\\ \u0026 \\frac{\\partial^2 f}{\\partial y^2}=f(y+1)-2f(y)+f(y-1) \\end{aligned} \\end{equation} $$ 那么得到上述二阶微分：\n$$ \\begin{equation} \\begin{aligned} \\nabla^2 f \u0026=\\frac{\\partial^2 f}{\\partial x^2}+\\frac{\\partial^2 f}{\\partial y^2}\\\\ \u0026=[f(x+1, y)+f(x-1, y)+f(x, y+1)+f(x, y-1)-4 f(x, y)] \\end{aligned} \\end{equation} $$ 相对应的系数即为相应的矩阵形式\n$$ \\begin{equation} \\left[\\begin{array}{ccc} 0 \u0026 1 \u0026 0 \\\\ 1 \u0026 -4 \u0026 1 \\\\ 0 \u0026 1 \u0026 0 \\end{array}\\right] \\end{equation} $$ 训练数值模型及其相应结果 下图计算结果中图A-D代表着不同数据集对应的混淆矩阵. 在基于深度学习的分类识别领域中，经常采用统计学中的混淆矩阵（confusion matrix）来评价分类器的性能. 它是一种特定的二维矩阵：列代表预测的类别；行代表实际的类别.\n对角线上的值表示预测正确的数量或比例；非对角线元素是预测错误的部分. 混淆矩阵的对角线值越高越好，表明许多正确的预测. 特别是在各分类数据的数量不平衡的情况下，混淆矩阵可以直观的显示分类模型对应各个类别的准确率.\n该系统相对于训练数据集的平均准确率为 $92.6±1.1$%，测试数据集平均准确率为 $86.3±4.3$%. 观察到该系统在ae元音上获得了近乎完美的预测性能，并且能够区分iy元音和ei元音，但准确性较低，特别是在来自测试数据集的未见样本中.\n总结 使用物理来执行计算的方法可能会成为模拟机器学习设备的新平台，相比通过计算机进行机器学习更自然、更有效地执行计算的潜力. 除此之外，这篇论文同样发人深省，在相对应的其它物理系统中仍然可以找到相似对应的结构，例如光学中的傍轴衍射方程和热传导方程，它们与波动方程都有相似的结构.\nReference Hughes, Tyler W., et al. \u0026ldquo;Wave physics as an analog recurrent neural network.\u0026rdquo; Science advances 5.12 (2019): eaay6946.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://wowking2018.github.io/posts/2023-01-10-wave-physics-as-an-analog-recurrent-neural-network/","summary":"RNN介绍 RNN(循环神经网络)是在自然语言处理NLP领域中最先被用起来的，比如，RNN可以为语言模型来建模. 循环神经网络种类繁多，最简单的","title":"Wave physics as an analog recurrent neural network"},{"content":"Introduction When it comes to deep learning, it is no doubt to encounter a concept called information entropy 1. My friend asked a question for me—So, what is entropy? In physics, entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature 2. Austrian physicist Boltzmann explained entropy as the degree of disorder or randomness in a system.\nMathematical derivation In this blog, I will show a kind of method presented by Frederick in his classical book Fundamentals of statistical and thermal physics. Clearly and step by step:\nImagine two thermal internal interaction between two macroscopic systems $A$ and $A^{\\prime}$, we shall denote the respective energies of these systems by $E$ and $E^{\\prime}$. Moreover, the combined system $A^{(0)} \\equiv A + A^{\\prime}$ is isolated and its total energy $E^{(0)}$ is therefore constant 3.\nIt follows that the the probability $P(E)$ of finding this combineed system in a configuration where $A$ has an energy is simply proportional to the number of states $\\Omega^{(0)}(E)$. This probability could also be written as\n$$ \\begin{equation} P(E) = \\frac{\\Omega ^{(0)}(E)}{\\Omega_{\\text{tot}}}, \\end{equation} $$\nwhere $\\Omega ^{(0)}_ {\\textrm{tot}}$ denotes the total number of states accessible to $A^{(0)}$. $\\Omega ^{(0)}_ {\\textrm{tot}}$ can be obtained by summing $\\Omega^{(0)}(E)$ over all possible energies $E$ of the system $A$, which means $\\Omega ^{(0)}_{\\text{tot}}$ is constant. In symbols this can be written as\n$$ \\begin{equation} P(E) = C \\Omega^{(0)}(E), \\end{equation}\\label{eq2} $$\nwhere $C$ is a constant of proportionality independent of $E$. Indeed, suppose that $A$ has an energy $E$, and $A^{\\prime}$ has the corresponding energy known as to be $E^{\\prime} = E^{(0)}-E$. In Eq. \\eqref{eq2}, $\\Omega^{(0)}(E)$ is simply given by the product\n$$ \\begin{equation} \\Omega^{(0)}(E)=\\Omega(E) \\Omega^{\\prime}\\left(E^{(0)}-E\\right). \\end{equation} $$\nCorrespondingly, system $A$ having an energy near $E$ is simply given by\n$$ \\begin{equation} P(E)=C \\Omega(E) \\Omega^{\\prime}\\left(E^{(0)}-E\\right). \\end{equation} $$\nTo locate the position of the maximum of $P(E)$, we need to find the value when\n$$ \\begin{equation} \\frac{\\partial \\ln P}{\\partial E}=0, \\end{equation} \\label{eq5} $$\nwhere\n$$ \\ln P(E)=\\ln C+\\ln \\Omega(E)+\\ln \\Omega^{\\prime}\\left(E^{\\prime}\\right). $$\nAs what we have said before, $E^{\\prime}=E^{(0)}-E$. Hence, Eq. \\eqref{eq5} becomes\n$$ \\begin{equation} \\frac{\\partial \\ln \\Omega(E)}{\\partial E} - \\frac{\\partial \\ln \\Omega^{\\prime}\\left(E^{\\prime}\\right)}{\\partial E^{\\prime}}=0, \\end{equation} $$\nwhere we have to introduced the definition\n$$ \\begin{equation} \\beta(E) \\equiv \\frac{\\partial \\ln \\Omega}{\\partial E}. \\end{equation} $$\nThen we get $\\beta(\\tilde{E}) = \\beta^{\\prime}(\\tilde{E}^{\\prime})$, where $\\tilde{E}$ and $\\tilde{E}^{\\prime}$ denote the corresponding energies of $A$ and $A^{\\prime}$ at the maximum, By its definition, the parameter $\\beta$ has the dimensions of a reciprocal energy. It is convenient to introduce a dimensionless parameter $T$ defined by writing $\\beta = (k T)^{-1}$, where $k$ is some positive sonstant having the dimensions of energy.\nLet me explain that why is $\\beta = (k T)^{-1}$? Like what we have learnt in thermodynamics,\nentropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature.\nThe corresponding connection can be shown like this\n$$ \\begin{equation} \\frac{1}{T} = \\frac{\\partial S}{\\partial E}, \\end{equation}\\label{eq8} $$\nwe usually use this definition $S \\equiv k \\ln \\Omega$, combine it with Eq. \\eqref{eq8}, we get\n$$ \\begin{equation} \\frac{1}{T} = \\frac{k \\partial \\Omega}{\\partial E} \\rightarrow \\frac{1}{kT} \\equiv \\beta \\equiv \\frac{\\partial \\ln \\Omega}{\\partial E}. \\end{equation} $$\nThe truth is that when entropy of two subsystems reaches the maximum value of the state is the most likely state for both systems.\nReference Shannon, Claude Elwood. \u0026ldquo;A mathematical theory of communication.\u0026rdquo; ACM SIGMOBILE mobile computing and communications review 5.1 (2001): 3-55.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWikipedia-Entropy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nReif, Frederick, and Stuart A. Rice. \u0026ldquo;Fundamentals of statistical and thermal physics.\u0026rdquo; Physics Today 20.12 (1967): 85-87.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://wowking2018.github.io/posts/2022-06-19-what-is-entropy/","summary":"Introduction When it comes to deep learning, it is no doubt to encounter a concept called information entropy 1. My friend asked a question for me—So, what is entropy? In physics, entropy is a concept that shows quotient of an infinitesimal amount of heat to the instantaneous temperature 2. Austrian physicist Boltzmann explained entropy as the degree of disorder or randomness in a system.\nMathematical derivation In this blog, I will show a kind of method presented by Frederick in his classical book Fundamentals of statistical and thermal physics.","title":"What is entropy?"},{"content":"绪论 1971 年，美国加州大学伯克利分校的科学家蔡少棠教授（Prof. Leon Chua）提出了忆阻器（Memristor）的概念. 从统计学的角度和完备性的概念，推断忆阻器的存在.\n电路方程中的基本变量有四个，即电压 $V$ 、电流 $I$ 、电荷 $q$ 与磁通量 $\\varphi$，四个变量对应六种关系，若将磁通量与电荷的关系定义为 ${\\rm{d}}\\varphi=M{\\rm{d}}q$ （其中 $M$ 为忆阻器阻值，$\\varphi$ 为电荷通量，$q$ 为电荷量），这便是第四个基本电路元件：记忆电阻器.\n数学模型 忆阻器模型根据电流电压特性分为理想忆阻器模型和实际忆阻器模型，惠普忆阻器是目前应用最普遍的理想忆阻器模型，惠普忆阻器的数学模型如公式所示\n$$ \\begin{equation} \\begin{aligned} \u0026v(t) =\\left(\\mathcal{R}_ {\\text {on }} \\frac{w(t)}{D}+\\mathcal{R}_ {\\text {off }}\\left(1-\\frac{w(t)}{D}\\right)\\right) i(t) \\\\ \u0026M(t) =\\mathcal{R}_ {\\text{off }}+\\left(\\mathcal{R}_ {\\text {on }}-\\mathcal{R}_ {\\text {off }}\\right) \\frac{w(t)}{D} \\end{aligned} \\end{equation} $$ 其中 $M(t)$ 为忆阻值， $\\mathcal{R}_ {\\text {on}}$ 和 $\\mathcal{R}_ {\\text{off}}$ 是忆阻器电阻的极限值， $w(t)$ 是含有氧原子空位的 $\\rm{TiO}_ {2-\\mathrm{x}}$ 厚度变化 率， $D$ 是薄膜的厚度， $\\mathcal{R}_ {\\text{on}}$ 是当 $\\mathrm{TiO}_ {2-x}$ 厚度达到最大值，即 $w(t)=D$ 时忆阻器的阻值， $\\mathcal{R}_ {\\text {off }}$ 是当 $\\mathrm{TiO}_ {2-\\mathrm{x}}$厚度达到最小值，即 $w(t)=0$ 时忆阻器的阻值.\n对于忆阻器模型有\n$$ \\begin{equation} \\frac{\\mathrm{d} w(t)}{\\mathrm{d} t}=\\frac{\\mu_ {\\mathrm{V}} R_ {\\mathrm{on}}}{D} i(t) \\end{equation} $$\n其中 $\\mu_ {\\mathrm{V}}$ 为平均离子迁移率，即氧空位的迁移率. 可以看出一定电流的作用下 $\\mathrm{TiO}_ {2-x}$ 厚度的变化率即忆 阻器阻值变化速率与 $\\mu_ {\\mathrm{V}}$ 和 $\\mathcal{R}_ {\\text {on}}$ 成正比，与 $D$ 成反比. 当 $w(0)=w(t)|_ {t=0} \\neq 0$ 时，可以得到\n$$ \\begin{equation} w(t)=\\frac{\\mu_ {\\mathrm{V}} \\mathcal{R}_ {\\mathrm{on}}}{D} q(t)+w(0) \\end{equation} $$\n可以写成\n$$ \\begin{equation} M(t)=M(0)+k q(t) \\end{equation} $$\n其中，常数为\n$$ \\begin{equation} k=\\frac{\\left(\\mathcal{R}_ {\\mathrm{on}}-\\mathcal{R}_ {\\mathrm{off}}\\right) \\mu_ {\\mathrm{V}} \\mathcal{R}_ {\\mathrm{on}}}{D^{2}} \\end{equation} $$\n当满足条件 $\\mathcal{R}_ {\\text {on }} \\ll \\mathcal{R}_ {\\text {off }}$ 可以化简为\n$$ \\begin{equation} M(q)=\\mathcal{R}_ {\\mathrm{off}}\\left(1-\\frac{\\mu_ {\\mathrm{V}} \\mathcal{R}_ {\\mathrm{on}}}{D^{2}} q(t)\\right) \\end{equation} $$\n此时忆阻器的忆阻值 $M(t)$ 与流过忆阻器的电荷 $q(t)$ 的关系为:\n$$ \\begin{equation} \\frac{\\mathcal{R}_ {\\text {off }}-M(t)}{k} \\leq q(t) \\leq \\frac{\\mathcal{R}_ {\\text {on }}-M(0)}{k} \\end{equation} $$\n即当 $q(t) \\geq \\frac{\\mathcal{R}_ {\\text {on }}-M(0)}{k}$ 时, $t$ 时刻忆阻器阻值 $M(t)=\\mathcal{R}_ {\\text {off }}$ 当 $q(t) \\leq \\frac{\\mathcal{R}_ {\\text {off }}-M(t)}{k}$ 时, $t$ 时刻忆阻器阻值 $M(t)=\\mathcal{R}_ {\\text {on }}$. 为了方便求解，定义 $x=w(t) / D$ ，最终我们可以化简为以下方程\n$$ \\begin{equation} \\begin{aligned} \\frac{d x}{d t} \u0026=k i(t), \\quad k=\\frac{\\mu_ {V} \\mathcal{R}_ {\\mathrm{on}}}{D^{2}} \\\\ v(t) \u0026=\\left(\\mathcal{R}_ {\\text {on }} x+\\mathcal{R}_ {\\text {off }}(1-x)\\right) i(t) \\end{aligned} \\end{equation} $$ 初始情况下由于具有初始电阻故 $x_ {0} \\neq 0$ ，并具有关系 $x_ {0}=\\frac{\\mathcal{R}_ {\\text {off }}-\\mathcal{R}_ {\\text {int }}}{\\mathcal{R}_ {\\text{off}}-\\mathcal{R}_ {\\text {on }}}$\n计算机模拟 根据公式可以看出, 忆阻器的忆阻特性由 $R_ {\\mathrm{on}}, R_ {\\text {off }}, M(\\mathrm{t}), M(0)$ 和 $D$ 决定，本文选定以下参数的忆阻器进行分析.\n$$ \\begin{aligned} \u0026\\mathcal{R}_ {\\text {on }}=0.1 \\mathrm{k} \\Omega, \\quad \\mathcal{R}_ {\\text {int }}=11 \\mathrm{k} \\Omega, \\quad \\mathcal{R}_ {\\text {off }}=16 \\mathrm{k} \\Omega \\\\ \u0026D=10 \\mathrm{~nm}, \\quad \\mu_ {\\mathrm{V}}=2 \\times 10^{-14} \\mathrm{~m}^{2} \\mathrm{~S}^{-1} \\mathrm{~V}^{-1}, \\quad \\omega_ {0}=1 \\mathrm{rad} / \\mathrm{s} \\end{aligned} $$ 如果施加正余弦性的电压幅值情况，那么将会形成3个对应的loop响应。\n","permalink":"https://wowking2018.github.io/posts/2021-06-29-memristor/","summary":"绪论 1971 年，美国加州大学伯克利分校的科学家蔡少棠教授（Prof. Leon Chua）提出了忆阻器（Memristor）的概念. 从统计学的角度和完备性的","title":"Memristor - the missing circuit element"}]